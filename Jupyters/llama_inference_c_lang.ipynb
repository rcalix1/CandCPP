{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67872357",
   "metadata": {},
   "source": [
    "\n",
    "## Inference for Llama-2 Transformer model in pure C\n",
    "\n",
    "* Author: Andrej Karpathy\n",
    "    \n",
    "* Link: https://github.com/karpathy/llama2.c/blob/master/run.c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcda68",
   "metadata": {},
   "source": [
    "\n",
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/* Inference for Llama-2 Transformer model in pure C */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <ctype.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include <string.h>\n",
    "#include <fcntl.h>\n",
    "#if defined _WIN32\n",
    "    #include \"win.h\"\n",
    "#else\n",
    "    #include <unistd.h>\n",
    "    #include <sys/mman.h>\n",
    "#endif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac9807",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    int dim;        // transformer dimension\n",
    "    int hidden_dim; // for ffn layers\n",
    "    int n_layers;   // number of layers\n",
    "    int n_heads;    // number of query heads\n",
    "    int n_kv_heads; // number of key/value heads (can be < query heads because of multiquery)\n",
    "    int vocab_size; // vocabulary size, usually 256 (byte-level)\n",
    "    int seq_len;    // max sequence length\n",
    "} Config;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15272e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    // token embedding table\n",
    "    float* token_embedding_table;    // (vocab_size, dim)\n",
    "    // weights for rmsnorms\n",
    "    float* rms_att_weight; // (layer, dim) rmsnorm weights\n",
    "    float* rms_ffn_weight; // (layer, dim)\n",
    "    // weights for matmuls. note dim == n_heads * head_size\n",
    "    float* wq; // (layer, dim, n_heads * head_size)\n",
    "    float* wk; // (layer, dim, n_kv_heads * head_size)\n",
    "    float* wv; // (layer, dim, n_kv_heads * head_size)\n",
    "    float* wo; // (layer, n_heads * head_size, dim)\n",
    "    // weights for ffn\n",
    "    float* w1; // (layer, hidden_dim, dim)\n",
    "    float* w2; // (layer, dim, hidden_dim)\n",
    "    float* w3; // (layer, hidden_dim, dim)\n",
    "    // final rmsnorm\n",
    "    float* rms_final_weight; // (dim,)\n",
    "    // (optional) classifier weights for the logits, on the last layer\n",
    "    float* wcls;\n",
    "} TransformerWeights;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    // current wave of activations\n",
    "    float *x; // activation at current time stamp (dim,)\n",
    "    float *xb; // same, but inside a residual branch (dim,)\n",
    "    float *xb2; // an additional buffer just for convenience (dim,)\n",
    "    float *hb; // buffer for hidden dimension in the ffn (hidden_dim,)\n",
    "    float *hb2; // buffer for hidden dimension in the ffn (hidden_dim,)\n",
    "    float *q; // query (dim,)\n",
    "    float *k; // key (dim,)\n",
    "    float *v; // value (dim,)\n",
    "    float *att; // buffer for scores/attention values (n_heads, seq_len)\n",
    "    float *logits; // output logits\n",
    "    // kv cache\n",
    "    float* key_cache;   // (layer, seq_len, dim)\n",
    "    float* value_cache; // (layer, seq_len, dim)\n",
    "} RunState;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a8838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    Config config; // the hyperparameters of the architecture (the blueprint)\n",
    "    TransformerWeights weights; // the weights of the model\n",
    "    RunState state; // buffers for the \"wave\" of activations in the forward pass\n",
    "    // some more state needed to properly clean up the memory mapping (sigh)\n",
    "    int fd; // file descriptor for memory mapping\n",
    "    float* data; // memory mapped data pointer\n",
    "    ssize_t file_size; // size of the checkpoint file in bytes\n",
    "} Transformer;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bd475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void malloc_run_state(RunState* s, Config* p) {\n",
    "    // we calloc instead of malloc to keep valgrind happy\n",
    "    int kv_dim     = (p->dim * p->n_kv_heads) / p->n_heads;\n",
    "    s->x           = calloc(p->dim, sizeof(float));\n",
    "    s->xb          = calloc(p->dim, sizeof(float));\n",
    "    s->xb2         = calloc(p->dim, sizeof(float));\n",
    "    s->hb          = calloc(p->hidden_dim, sizeof(float));\n",
    "    s->hb2         = calloc(p->hidden_dim, sizeof(float));\n",
    "    s->q           = calloc(p->dim, sizeof(float));\n",
    "    s->key_cache   = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n",
    "    s->value_cache = calloc(p->n_layers * p->seq_len * kv_dim, sizeof(float));\n",
    "    s->att         = calloc(p->n_heads * p->seq_len, sizeof(float));\n",
    "    s->logits      = calloc(p->vocab_size, sizeof(float));\n",
    "    // ensure all mallocs went fine\n",
    "    if (!s->x || !s->xb || !s->xb2 || !s->hb || !s->hb2 || !s->q\n",
    "     || !s->key_cache || !s->value_cache || !s->att || !s->logits) {\n",
    "        fprintf(stderr, \"malloc failed!\\n\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void free_run_state(RunState* s) {\n",
    "    free(s->x);\n",
    "    free(s->xb);\n",
    "    free(s->xb2);\n",
    "    free(s->hb);\n",
    "    free(s->hb2);\n",
    "    free(s->q);\n",
    "    free(s->att);\n",
    "    free(s->logits);\n",
    "    free(s->key_cache);\n",
    "    free(s->value_cache);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void memory_map_weights(\n",
    "             TransformerWeights *w, \n",
    "             Config* p, \n",
    "             float* ptr, \n",
    "             int shared_weights) \n",
    "{\n",
    "    int head_size = p->dim / p->n_heads;\n",
    "    // make sure the multiplications below are done in 64bit to fit the parameter counts of 13B+ models\n",
    "    unsigned long long n_layers = p->n_layers;\n",
    "    w->token_embedding_table = ptr;\n",
    "    ptr += p->vocab_size * p->dim;\n",
    "    w->rms_att_weight = ptr;\n",
    "    ptr += n_layers * p->dim;\n",
    "    w->wq = ptr;\n",
    "    ptr += n_layers * p->dim * (p->n_heads * head_size);\n",
    "    w->wk = ptr;\n",
    "    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n",
    "    w->wv = ptr;\n",
    "    ptr += n_layers * p->dim * (p->n_kv_heads * head_size);\n",
    "    w->wo = ptr;\n",
    "    ptr += n_layers * (p->n_heads * head_size) * p->dim;\n",
    "    w->rms_ffn_weight = ptr;\n",
    "    ptr += n_layers * p->dim;\n",
    "    w->w1 = ptr;\n",
    "    ptr += n_layers * p->dim * p->hidden_dim;\n",
    "    w->w2 = ptr;\n",
    "    ptr += n_layers * p->hidden_dim * p->dim;\n",
    "    w->w3 = ptr;\n",
    "    ptr += n_layers * p->dim * p->hidden_dim;\n",
    "    w->rms_final_weight = ptr;\n",
    "    ptr += p->dim;\n",
    "    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_real (for RoPE)\n",
    "    ptr += p->seq_len * head_size / 2; // skip what used to be freq_cis_imag (for RoPE)\n",
    "    w->wcls = shared_weights ? w->token_embedding_table : ptr;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d66223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void read_checkpoint(\n",
    "       char* checkpoint, \n",
    "       Config* config, \n",
    "       TransformerWeights* weights,\n",
    "       int* fd, \n",
    "       float** data, \n",
    "       ssize_t* file_size) \n",
    "{\n",
    "    FILE *file = fopen(checkpoint, \"rb\");\n",
    "    if (!file) { fprintf(stderr, \"Couldn't open file %s\\n\", checkpoint); exit(EXIT_FAILURE); }\n",
    "    // read in the config header\n",
    "    if (fread(config, sizeof(Config), 1, file) != 1) { exit(EXIT_FAILURE); }\n",
    "    // negative vocab size is hacky way of signaling unshared weights. bit yikes.\n",
    "    int shared_weights = config->vocab_size > 0 ? 1 : 0;\n",
    "    config->vocab_size = abs(config->vocab_size);\n",
    "    // figure out the file size\n",
    "    fseek(file, 0, SEEK_END); // move file pointer to end of file\n",
    "    *file_size = ftell(file); // get the file size, in bytes\n",
    "    fclose(file);\n",
    "    // memory map the Transformer weights into the data pointer\n",
    "    *fd = open(checkpoint, O_RDONLY); // open in read only mode\n",
    "    if (*fd == -1) { fprintf(stderr, \"open failed!\\n\"); exit(EXIT_FAILURE); }\n",
    "    *data = mmap(NULL, *file_size, PROT_READ, MAP_PRIVATE, *fd, 0);\n",
    "    if (*data == MAP_FAILED) { fprintf(stderr, \"mmap failed!\\n\"); exit(EXIT_FAILURE); }\n",
    "    float* weights_ptr = *data + sizeof(Config)/sizeof(float);\n",
    "    memory_map_weights(weights, config, weights_ptr, shared_weights);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void build_transformer(Transformer *t, char* checkpoint_path) {\n",
    "    // read in the Config and the Weights from the checkpoint\n",
    "    read_checkpoint(checkpoint_path, &t->config, &t->weights, &t->fd, &t->data, &t->file_size);\n",
    "    // allocate the RunState buffers\n",
    "    malloc_run_state(&t->state, &t->config);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void free_transformer(Transformer* t) {\n",
    "    // close the memory mapping\n",
    "    if (t->data != MAP_FAILED) { munmap(t->data, t->file_size); }\n",
    "    if (t->fd != -1) { close(t->fd); }\n",
    "    // free the RunState buffers\n",
    "    free_run_state(&t->state);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b191998",
   "metadata": {},
   "source": [
    "\n",
    "## Neural net blocks; the dynamics of the Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void rmsnorm(float* o, float* x, float* weight, int size) {\n",
    "    // calculate sum of squares\n",
    "    float ss = 0.0f;\n",
    "    for (int j = 0; j < size; j++) {\n",
    "        ss += x[j] * x[j];\n",
    "    }\n",
    "    ss /= size;\n",
    "    ss += 1e-5f;\n",
    "    ss  = 1.0f / sqrtf(ss);\n",
    "    // normalize and scale\n",
    "    for (int j = 0; j < size; j++) {\n",
    "        o[j] = weight[j] * (ss * x[j]);\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d295661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void softmax(float* x, int size) \n",
    "{\n",
    "    // find max value (for numerical stability)\n",
    "    float max_val = x[0];\n",
    "    for (int i = 1; i < size; i++) {\n",
    "        if (x[i] > max_val) {\n",
    "            max_val = x[i];\n",
    "        }\n",
    "    }\n",
    "    // exp and sum\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        x[i] = expf(x[i] - max_val);\n",
    "        sum += x[i];\n",
    "    }\n",
    "    // normalize\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        x[i] /= sum;\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d258d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void matmul(float* xout, float* x, float* w, int n, int d) {\n",
    "    // W (d,n) @ x (n,) -> xout (d,)\n",
    "    // by far the most amount of time is spent inside this little function\n",
    "    int i;\n",
    "    #pragma omp parallel for private(i)\n",
    "    for (i = 0; i < d; i++) {\n",
    "        float val = 0.0f;\n",
    "        for (int j = 0; j < n; j++) {\n",
    "            val += w[i * n + j] * x[j];\n",
    "        }\n",
    "        xout[i] = val;\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d576ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "float* forward(\n",
    "        Transformer* transformer, \n",
    "        int token, \n",
    "        int pos) \n",
    "{\n",
    "\n",
    "    // a few convenience variables\n",
    "    Config* p             = &transformer->config;\n",
    "    TransformerWeights* w = &transformer->weights;\n",
    "    RunState* s           = &transformer->state;\n",
    "    float *x              = s->x;\n",
    "    int dim               = p->dim;\n",
    "    int kv_dim            = (p->dim * p->n_kv_heads) / p->n_heads;\n",
    "    int kv_mul = p->n_heads / p->n_kv_heads; // integer multiplier of the kv sharing in multiquery\n",
    "    int hidden_dim        =  p->hidden_dim;\n",
    "    int head_size         = dim / p->n_heads;\n",
    "\n",
    "    // copy the token embedding into x\n",
    "    float* content_row = w->token_embedding_table + token * dim;\n",
    "    memcpy(x, content_row, dim*sizeof(*x));\n",
    "\n",
    "    // forward all the layers\n",
    "    for(unsigned long long l = 0; l < p->n_layers; l++) {\n",
    "\n",
    "        // attention rmsnorm\n",
    "        rmsnorm(s->xb, x, w->rms_att_weight + l*dim, dim);\n",
    "\n",
    "        // key and value point to the kv cache\n",
    "        int loff = l * p->seq_len * kv_dim; // kv cache layer offset for convenience\n",
    "        s->k     = s->key_cache + loff + pos * kv_dim;\n",
    "        s->v     = s->value_cache + loff + pos * kv_dim;\n",
    "\n",
    "        // qkv matmuls for this position\n",
    "        matmul(s->q, s->xb, w->wq + l*dim*dim,    dim, dim);\n",
    "        matmul(s->k, s->xb, w->wk + l*dim*kv_dim, dim, kv_dim);\n",
    "        matmul(s->v, s->xb, w->wv + l*dim*kv_dim, dim, kv_dim);\n",
    "\n",
    "        // RoPE relative positional encoding: complex-valued rotate q and k in each head\n",
    "        for (int i = 0; i < dim; i+=2) {\n",
    "            int head_dim = i % head_size;\n",
    "            float freq = 1.0f / powf(10000.0f, head_dim / (float)head_size);\n",
    "            float val  = pos * freq;\n",
    "            float fcr  = cosf(val);\n",
    "            float fci  = sinf(val);\n",
    "            int rotn   = i < kv_dim ? 2 : 1; // how many vectors? 2 = q & k, 1 = q only\n",
    "            for (int v = 0; v < rotn; v++) {\n",
    "                float* vec = v == 0 ? s->q : s->k; // the vector to rotate (query or key)\n",
    "                float v0   = vec[i];\n",
    "                float v1   = vec[i+1];\n",
    "                vec[i]     = v0 * fcr - v1 * fci;\n",
    "                vec[i+1]   = v0 * fci + v1 * fcr;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // multihead attention. iterate over all heads\n",
    "        int h;\n",
    "        #pragma omp parallel for private(h)\n",
    "        for (h = 0; h < p->n_heads; h++) {\n",
    "            // get the query vector for this head\n",
    "            float* q = s->q + h * head_size;\n",
    "            // attention scores for this head\n",
    "            float* att = s->att + h * p->seq_len;\n",
    "            // iterate over all timesteps, including the current one\n",
    "            for (int t = 0; t <= pos; t++) {\n",
    "                // get the key vector for this head and at this timestep\n",
    "                float* k = s->key_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n",
    "                // calculate the attention score as the dot product of q and k\n",
    "                float score = 0.0f;\n",
    "                for (int i = 0; i < head_size; i++) {\n",
    "                    score += q[i] * k[i];\n",
    "                }\n",
    "                score /= sqrtf(head_size);\n",
    "                // save the score to the attention buffer\n",
    "                att[t] = score;\n",
    "            }\n",
    "\n",
    "            // softmax the scores to get attention weights, from 0..pos inclusively\n",
    "            softmax(att, pos + 1);\n",
    "\n",
    "            // weighted sum of the values, store back into xb\n",
    "            float* xb = s->xb + h * head_size;\n",
    "            memset(xb, 0, head_size * sizeof(float));\n",
    "            for (int t = 0; t <= pos; t++) {\n",
    "                // get the value vector for this head and at this timestep\n",
    "                float* v = s->value_cache + loff + t * kv_dim + (h / kv_mul) * head_size;\n",
    "                // get the attention weight for this timestep\n",
    "                float a  = att[t];\n",
    "                // accumulate the weighted value into xb\n",
    "                for (int i = 0; i < head_size; i++) {\n",
    "                    xb[i] += a * v[i];\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // final matmul to get the output of the attention\n",
    "        matmul(s->xb2, s->xb, w->wo + l*dim*dim, dim, dim);\n",
    "\n",
    "        // residual connection back into x\n",
    "        for (int i = 0; i < dim; i++) {\n",
    "            x[i] += s->xb2[i];\n",
    "        }\n",
    "\n",
    "        // ffn rmsnorm\n",
    "        rmsnorm(s->xb, x, w->rms_ffn_weight + l*dim, dim);\n",
    "\n",
    "        // Now for FFN in PyTorch we have: self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "        // first calculate self.w1(x) and self.w3(x)\n",
    "        matmul(s->hb,  s->xb, w->w1 + l*dim*hidden_dim, dim, hidden_dim);\n",
    "        matmul(s->hb2, s->xb, w->w3 + l*dim*hidden_dim, dim, hidden_dim);\n",
    "\n",
    "        // SwiGLU non-linearity\n",
    "        for (int i = 0; i < hidden_dim; i++) \n",
    "        {\n",
    "            float val = s->hb[i];\n",
    "            // silu(x)=x*σ(x), where σ(x) is the logistic sigmoid\n",
    "            val *= (1.0f / (1.0f + expf(-val)));\n",
    "            // elementwise multiply with w3(x)\n",
    "            val *= s->hb2[i];\n",
    "            s->hb[i] = val;\n",
    "        }\n",
    "\n",
    "        // final matmul to get the output of the ffn\n",
    "        matmul(s->xb, s->hb, w->w2 + l*dim*hidden_dim, hidden_dim, dim);\n",
    "\n",
    "        // residual connection\n",
    "        for (int i = 0; i < dim; i++) {\n",
    "            x[i] += s->xb[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // final rmsnorm\n",
    "    rmsnorm(x, x, w->rms_final_weight, dim);\n",
    "\n",
    "    // classifier into logits\n",
    "    matmul(s->logits, x, w->wcls, p->dim, p->vocab_size);\n",
    "    return s->logits;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36eef8",
   "metadata": {},
   "source": [
    "\n",
    "## Tokenizer\n",
    "\n",
    "* The Byte Pair Encoding (BPE) Tokenizer that translates strings <-> tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    char *str;\n",
    "    int id;\n",
    "} TokenIndex;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a733c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    char** vocab;\n",
    "    float* vocab_scores;\n",
    "    TokenIndex *sorted_vocab;\n",
    "    int vocab_size;\n",
    "    unsigned int max_token_length;\n",
    "    unsigned char byte_pieces[512]; // stores all single-byte strings\n",
    "} Tokenizer;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2511ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int compare_tokens(const void *a, const void *b) {\n",
    "    return strcmp(((TokenIndex*)a)->str, ((TokenIndex*)b)->str);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void build_tokenizer(\n",
    "           Tokenizer* t, \n",
    "           char* tokenizer_path, \n",
    "           int vocab_size) \n",
    "{\n",
    "    // i should have written the vocab_size into the tokenizer file... sigh\n",
    "    t->vocab_size   = vocab_size;\n",
    "    // malloc space to hold the scores and the strings\n",
    "    t->vocab = (char**)malloc(vocab_size * sizeof(char*));\n",
    "    t->vocab_scores = (float*)malloc(vocab_size * sizeof(float));\n",
    "    t->sorted_vocab = NULL; // initialized lazily\n",
    "    for (int i = 0; i < 256; i++) {\n",
    "        t->byte_pieces[i * 2]     = (unsigned char)i;\n",
    "        t->byte_pieces[i * 2 + 1] = '\\0';\n",
    "    }\n",
    "    \n",
    "    // read in the file\n",
    "    FILE *file = fopen(tokenizer_path, \"rb\");\n",
    "    if (!file) { fprintf(stderr, \"couldn't load %s\\n\", tokenizer_path); exit(EXIT_FAILURE); }\n",
    "    if (fread(&t->max_token_length, sizeof(int), 1, file) != 1) \n",
    "    { \n",
    "        fprintf(stderr, \"failed read\\n\"); \n",
    "        exit(EXIT_FAILURE); \n",
    "    }\n",
    "    int len;\n",
    "    for (int i = 0; i < vocab_size; i++) {\n",
    "        if (fread(t->vocab_scores + i, sizeof(float), 1, file) != 1) \n",
    "        { \n",
    "            fprintf(stderr, \"failed read\\n\"); \n",
    "            exit(EXIT_FAILURE);\n",
    "        }\n",
    "        if (fread(&len, sizeof(int), 1, file) != 1) \n",
    "        { \n",
    "            fprintf(stderr, \"failed read\\n\"); \n",
    "            exit(EXIT_FAILURE); \n",
    "        }\n",
    "        t->vocab[i] = (char *)malloc(len + 1);\n",
    "        if (fread(t->vocab[i], len, 1, file) != 1) \n",
    "        { \n",
    "            fprintf(stderr, \"failed read\\n\"); \n",
    "            exit(EXIT_FAILURE); \n",
    "        }\n",
    "        t->vocab[i][len] = '\\0'; // add the string terminating token\n",
    "    }\n",
    "    fclose(file);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a623b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void free_tokenizer(Tokenizer* t) \n",
    "{\n",
    "    for (int i = 0; i < t->vocab_size; i++) { free(t->vocab[i]); }\n",
    "    free(t->vocab);\n",
    "    free(t->vocab_scores);\n",
    "    free(t->sorted_vocab);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char* decode(\n",
    "       Tokenizer* t, \n",
    "       int prev_token, \n",
    "       int token) \n",
    "{\n",
    "    char *piece = t->vocab[token];\n",
    "    \n",
    "    // following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n",
    "    if (prev_token == 1 && piece[0] == ' ') { piece++; }\n",
    "    // careful, some tokens designate raw bytes, and look like e.g. '<0x01>'\n",
    "    // parse this and convert and return the actual byte\n",
    "    unsigned char byte_val;\n",
    "    if (sscanf(piece, \"<0x%02hhX>\", &byte_val) == 1) \n",
    "    {\n",
    "        piece = (char*)t->byte_pieces + byte_val * 2;\n",
    "    }\n",
    "    return piece;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ce614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void safe_printf(char *piece) \n",
    "{\n",
    "    // piece might be a raw byte token, and we only want to print printable chars or whitespace\n",
    "    // because some of the other bytes can be various control codes, backspace, etc.\n",
    "    if (piece == NULL) { return; }\n",
    "    if (piece[0] == '\\0') { return; }\n",
    "    if (piece[1] == '\\0') {\n",
    "        unsigned char byte_val = piece[0];\n",
    "        if (!(isprint(byte_val) || isspace(byte_val))) {\n",
    "            return; // bad byte, don't print it\n",
    "        }\n",
    "    }\n",
    "    printf(\"%s\", piece);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca208122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "int str_lookup(\n",
    "        char *str, \n",
    "        TokenIndex *sorted_vocab, \n",
    "        int vocab_size) \n",
    "{\n",
    "    // efficiently find the perfect match for str in vocab, return its index or -1 if not found\n",
    "    TokenIndex tok  = { .str = str }; // acts as the key to search for\n",
    "    TokenIndex *res = bsearch(&tok, sorted_vocab, vocab_size, sizeof(TokenIndex), compare_tokens);\n",
    "    return res != NULL ? res->id : -1;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39522a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void encode(\n",
    "    Tokenizer* t, \n",
    "    char *text, \n",
    "    int8_t bos, \n",
    "    int8_t eos, \n",
    "    int *tokens, \n",
    "    int *n_tokens) \n",
    "{\n",
    "    // encode the string text (input) into an upper-bound preallocated tokens[] array\n",
    "    // bos != 0 means prepend the BOS token (=1), eos != 0 means append the EOS token (=2)\n",
    "    if (text == NULL) { fprintf(stderr, \"cannot encode NULL text\\n\"); exit(EXIT_FAILURE); }\n",
    "\n",
    "    if (t->sorted_vocab == NULL) {\n",
    "        // lazily malloc and sort the vocabulary\n",
    "        t->sorted_vocab = malloc(t->vocab_size * sizeof(TokenIndex));\n",
    "        for (int i = 0; i < t->vocab_size; i++) {\n",
    "            t->sorted_vocab[i].str = t->vocab[i];\n",
    "            t->sorted_vocab[i].id  = i;\n",
    "        }\n",
    "        qsort(t->sorted_vocab, t->vocab_size, sizeof(TokenIndex), compare_tokens);\n",
    "    }\n",
    "\n",
    "    // create a temporary buffer that will store merge candidates of always two consecutive tokens\n",
    "    // *2 for concat, +1 for null terminator +2 for UTF8 (in case max_token_length is 1)\n",
    "    char* str_buffer = malloc((t->max_token_length*2 +1 +2) * sizeof(char));\n",
    "    size_t str_len   = 0;\n",
    "\n",
    "    // start at 0 tokens\n",
    "    *n_tokens = 0;\n",
    "\n",
    "    // add optional BOS (=1) token, if desired\n",
    "    if (bos) tokens[(*n_tokens)++] = 1;\n",
    "\n",
    "    // add_dummy_prefix is true by default\n",
    "    // so prepend a dummy prefix token to the input string, but only if text != \"\"\n",
    "    // TODO: pretty sure this isn't correct in the general case but I don't have the\n",
    "    // energy to read more of the sentencepiece code to figure out what it's doing\n",
    "    if (text[0] != '\\0') {\n",
    "        int dummy_prefix      = str_lookup(\" \", t->sorted_vocab, t->vocab_size);\n",
    "        tokens[(*n_tokens)++] = dummy_prefix;\n",
    "    }\n",
    "\n",
    "    // Okay UTF-8 time. This will get messy. Here is the reference from Wikipedia:\n",
    "    // Code point ↔ UTF-8 conversion\n",
    "    // First code point\tLast code point\tByte 1\tByte 2\tByte 3\tByte 4\n",
    "    // U+0000\tU+007F\t    0xxxxxxx\n",
    "    // U+0080\tU+07FF\t    110xxxxx\t10xxxxxx\n",
    "    // U+0800\tU+FFFF\t    1110xxxx\t10xxxxxx\t10xxxxxx\n",
    "    // U+10000\tU+10FFFF    11110xxx\t10xxxxxx\t10xxxxxx\t10xxxxxx\n",
    "\n",
    "    // process the raw (UTF-8) byte sequence of the input string\n",
    "    for (char *c = text; *c != '\\0'; c++) {\n",
    "\n",
    "        // reset buffer if the current byte is ASCII or a leading byte\n",
    "        // 0xC0 is 11000000, so (*c & 0xC0) keeps the first 2 bits and zeros the rest\n",
    "        // 0x80 is 10000000\n",
    "        // in UTF-8, all continuation bytes start with \"10\" in first two bits\n",
    "        // so in English this is: \"if this byte is not a continuation byte\"\n",
    "        if ((*c & 0xC0) != 0x80) {\n",
    "            // this byte must be either a leading byte (11...) or an ASCII char (0x...)\n",
    "            // => reset our location, as we're starting a new UTF-8 codepoint\n",
    "            str_len = 0;\n",
    "        }\n",
    "\n",
    "        // append the current byte to the buffer\n",
    "        str_buffer[str_len++] = *c; // ++ is post-increment, incremented after this line\n",
    "        str_buffer[str_len]   = '\\0';\n",
    "\n",
    "        // while the next character is a continuation byte, continue appending\n",
    "        // but if there are too many of them, just stop to avoid overruning str_buffer size.\n",
    "        if ((*(c+1) & 0xC0) == 0x80 && str_len < 4) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        // ok c+1 is not a continuation byte, so we've read in a full codepoint\n",
    "        int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n",
    "\n",
    "        if (id != -1) {\n",
    "            // we found this codepoint in vocab, add it as a token\n",
    "            tokens[(*n_tokens)++] = id;\n",
    "        } else {\n",
    "            // byte_fallback encoding: just encode each byte as a token\n",
    "            // +3 is here because the first 3 vocab elements are <unk>, <s>, </s>\n",
    "            // so the individual bytes only start at index 3\n",
    "            for (int i=0; i < str_len; i++) {\n",
    "                tokens[(*n_tokens)++] = (unsigned char)str_buffer[i] + 3;\n",
    "            }\n",
    "        }\n",
    "        str_len = 0; // protect against a sequence of stray UTF8 continuation bytes\n",
    "    }\n",
    "\n",
    "    // merge the best consecutive pair each iteration, according the scores in vocab_scores\n",
    "    while (1) {\n",
    "        float best_score = -1e10;\n",
    "        int best_id      = -1;\n",
    "        int best_idx     = -1;\n",
    "\n",
    "        for (int i=0; i < (*n_tokens-1); i++) \n",
    "        {\n",
    "            // check if we can merge the pair (tokens[i], tokens[i+1])\n",
    "            sprintf(str_buffer, \"%s%s\", t->vocab[tokens[i]], t->vocab[tokens[i+1]]);\n",
    "            int id = str_lookup(str_buffer, t->sorted_vocab, t->vocab_size);\n",
    "            if (id != -1 && t->vocab_scores[id] > best_score) \n",
    "            {\n",
    "                // this merge pair exists in vocab! record its score and position\n",
    "                best_score = t->vocab_scores[id];\n",
    "                best_id    = id;\n",
    "                best_idx   = i;\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if (best_idx == -1) {\n",
    "            break; // we couldn't find any more pairs to merge, so we're done\n",
    "        }\n",
    "\n",
    "        // merge the consecutive pair (best_idx, best_idx+1) into new token best_id\n",
    "        tokens[best_idx] = best_id;\n",
    "        // delete token at position best_idx+1, shift the entire sequence back 1\n",
    "        for (int i = best_idx+1; i < (*n_tokens-1); i++) {\n",
    "            tokens[i] = tokens[i+1];\n",
    "        }\n",
    "        (*n_tokens)--; // token length decreased\n",
    "    }\n",
    "\n",
    "    // add optional EOS (=2) token, if desired\n",
    "    if (eos) tokens[(*n_tokens)++] = 2;\n",
    "\n",
    "    free(str_buffer);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fdad40",
   "metadata": {},
   "source": [
    "\n",
    "## The Sampler, which takes logits and returns a sampled token\n",
    "\n",
    "* Sampling can be done in a few ways: greedy argmax, sampling, top-p sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae265c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    float prob;\n",
    "    int index;\n",
    "} ProbIndex; // struct used when sorting probabilities during top-p sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5a176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typedef struct {\n",
    "    int vocab_size;\n",
    "    ProbIndex* probindex; // buffer used in top-p sampling\n",
    "    float temperature;\n",
    "    float topp;\n",
    "    unsigned long long rng_state;\n",
    "} Sampler;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ef3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int sample_argmax(float* probabilities, int n) {\n",
    "    // return the index that has the highest probability\n",
    "    int max_i   = 0;\n",
    "    float max_p = probabilities[0];\n",
    "    for (int i = 1; i < n; i++) \n",
    "    {\n",
    "        if (probabilities[i] > max_p) {\n",
    "            max_i = i;\n",
    "            max_p = probabilities[i];\n",
    "        }\n",
    "    }\n",
    "    return max_i;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "int sample_mult(\n",
    "         float* probabilities, \n",
    "         int n, \n",
    "         float coin ) \n",
    "{\n",
    "    // sample index from probabilities (they must sum to 1!)\n",
    "    // coin is a random number in [0, 1], usually from random_f32()\n",
    "    float cdf = 0.0f;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        cdf += probabilities[i];\n",
    "        if (coin < cdf) {\n",
    "            return i;\n",
    "        }\n",
    "    }\n",
    "    return n - 1; // in case of rounding errors\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int compare(\n",
    "       const void* a, \n",
    "       const void* b) \n",
    "{\n",
    "    ProbIndex* a_ = (ProbIndex*) a;\n",
    "    ProbIndex* b_ = (ProbIndex*) b;\n",
    "    if (a_->prob > b_->prob) return -1;\n",
    "    if (a_->prob < b_->prob) return  1;\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe56bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "int sample_topp(\n",
    "      float* probabilities, \n",
    "      int n, \n",
    "      float topp, \n",
    "      ProbIndex* probindex, \n",
    "      float coin) \n",
    "{\n",
    "    // top-p sampling (or \"nucleus sampling\") samples from the smallest set of\n",
    "    // tokens that exceed probability topp. This way we never sample tokens that\n",
    "    // have very low probabilities and are less likely to go \"off the rails\".\n",
    "    // coin is a random number in [0, 1], usually from random_f32()\n",
    "\n",
    "    int n0 = 0;\n",
    "    // quicksort indices in descending order of probabilities\n",
    "    // values smaller than (1 - topp) / (n - 1) cannot be part of the result\n",
    "    // so for efficiency we crop these out as candidates before sorting\n",
    "    const float cutoff = (1.0f - topp) / (n - 1);\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (probabilities[i] >= cutoff) {\n",
    "            probindex[n0].index = i;\n",
    "            probindex[n0].prob  = probabilities[i];\n",
    "            n0++;\n",
    "        }\n",
    "    }\n",
    "    qsort(probindex, n0, sizeof(ProbIndex), compare);\n",
    "\n",
    "    // truncate the list where cumulative probability exceeds topp\n",
    "    float cumulative_prob = 0.0f;\n",
    "    int last_idx          = n0 - 1; // in case of rounding errors consider all elements\n",
    "    for (int i = 0; i < n0; i++) {\n",
    "        cumulative_prob += probindex[i].prob;\n",
    "        if (cumulative_prob > topp) {\n",
    "            last_idx = i;\n",
    "            break; // we've exceeded topp by including last_idx\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // sample from the truncated list\n",
    "    float r   = coin * cumulative_prob;\n",
    "    float cdf = 0.0f;\n",
    "    for (int i = 0; i <= last_idx; i++) {\n",
    "        cdf += probindex[i].prob;\n",
    "        if (r < cdf) {\n",
    "            return probindex[i].index;\n",
    "        }\n",
    "    }\n",
    "    return probindex[last_idx].index; // in case of rounding errors\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void build_sampler(\n",
    "       Sampler* sampler, \n",
    "       int vocab_size, \n",
    "       float temperature, \n",
    "       float topp, \n",
    "       unsigned long long rng_seed) \n",
    "{\n",
    "    sampler->vocab_size  = vocab_size;\n",
    "    sampler->temperature = temperature;\n",
    "    sampler->topp        = topp;\n",
    "    sampler->rng_state   = rng_seed;\n",
    "    // buffer only used with nucleus sampling; may not need but it's ~small\n",
    "    sampler->probindex   = malloc(sampler->vocab_size * sizeof(ProbIndex));\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void free_sampler(Sampler* sampler) {\n",
    "    free(sampler->probindex);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unsigned int random_u32(unsigned long long *state) {\n",
    "    // xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n",
    "    *state ^= *state >> 12;\n",
    "    *state ^= *state << 25;\n",
    "    *state ^= *state >> 27;\n",
    "    return (*state * 0x2545F4914F6CDD1Dull) >> 32;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "float random_f32(unsigned long long *state) { // random float32 in [0,1)\n",
    "    return (random_u32(state) >> 8) / 16777216.0f;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "int sample(\n",
    "     Sampler* sampler, \n",
    "     float* logits) \n",
    "{\n",
    "    // sample the token given the logits and some hyperparameters\n",
    "    int next;\n",
    "    if (sampler->temperature == 0.0f) {\n",
    "        // greedy argmax sampling: take the token with the highest probability\n",
    "        next = sample_argmax(logits, sampler->vocab_size);\n",
    "    } else {\n",
    "        // apply the temperature to the logits\n",
    "        for (int q=0; q<sampler->vocab_size; q++) { logits[q] /= sampler->temperature; }\n",
    "        // apply softmax to the logits to get the probabilities for next token\n",
    "        softmax(logits, sampler->vocab_size);\n",
    "        // flip a (float) coin (this is our source of entropy for sampling)\n",
    "        float coin = random_f32(&sampler->rng_state);\n",
    "        // we sample from this distribution to get the next token\n",
    "        if (sampler->topp <= 0 || sampler->topp >= 1) {\n",
    "            // simply sample from the predicted probability distribution\n",
    "            next = sample_mult(logits, sampler->vocab_size, coin);\n",
    "        } else {\n",
    "            // top-p (nucleus) sampling, clamping the least likely tokens to zero\n",
    "            next = sample_topp(logits, sampler->vocab_size, sampler->topp, sampler->probindex, coin);\n",
    "        }\n",
    "    }\n",
    "    return next;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0677d1",
   "metadata": {},
   "source": [
    "\n",
    "## Utilities\n",
    "\n",
    "* time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "long time_in_ms() {\n",
    "    // return time in milliseconds, for benchmarking the model speed\n",
    "    struct timespec time;\n",
    "    clock_gettime(CLOCK_REALTIME, &time);\n",
    "    return time.tv_sec * 1000 + time.tv_nsec / 1000000;\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b0c11",
   "metadata": {},
   "source": [
    "\n",
    "## Generation loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void generate(\n",
    "    Transformer *transformer, \n",
    "    Tokenizer *tokenizer, \n",
    "    Sampler *sampler, \n",
    "    char *prompt, \n",
    "    int steps) \n",
    "{\n",
    "    char *empty_prompt = \"\";\n",
    "    if (prompt == NULL) { prompt = empty_prompt; }\n",
    "\n",
    "    // encode the (string) prompt into tokens sequence\n",
    "    int num_prompt_tokens = 0;\n",
    "    int* prompt_tokens = (int*)malloc((strlen(prompt)+3) * sizeof(int)); // +3 for '\\0', ?BOS, ?EOS\n",
    "    encode(tokenizer, prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n",
    "    if (num_prompt_tokens < 1) {\n",
    "        fprintf(stderr, \"something is wrong, expected at least 1 prompt token\\n\");\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    // start the main loop\n",
    "    long start = 0;     // used to time our code, only initialized after first iteration\n",
    "    int next;           // will store the next token in the sequence\n",
    "    int token  = prompt_tokens[0]; // kick off with the first token in the prompt\n",
    "    int pos    = 0;     // position in the sequence\n",
    "    while (pos < steps) {\n",
    "\n",
    "        // forward the transformer to get logits for the next token\n",
    "        float* logits = forward(transformer, token, pos);\n",
    "\n",
    "        // advance the state machine\n",
    "        if (pos < num_prompt_tokens - 1) {\n",
    "            // if we are still processing the input prompt, force the next prompt token\n",
    "            next = prompt_tokens[pos + 1];\n",
    "        } else {\n",
    "            // otherwise sample the next token from the logits\n",
    "            next = sample(sampler, logits);\n",
    "        }\n",
    "        pos++;\n",
    "\n",
    "        // data-dependent terminating condition: the BOS (=1) token delimits sequences\n",
    "        if (next == 1) { break; }\n",
    "\n",
    "        // print the token as string, decode it with the Tokenizer object\n",
    "        char* piece = decode(tokenizer, token, next);\n",
    "        safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n",
    "        fflush(stdout);\n",
    "        token = next;\n",
    "\n",
    "        // init the timer here because the first iteration can be slower\n",
    "        if (start == 0) { start = time_in_ms(); }\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // report achieved tok/s (pos-1 because the timer starts after first iteration)\n",
    "    if (pos > 1) {\n",
    "        long end = time_in_ms();\n",
    "        fprintf(stderr, \"achieved tok/s: %f\\n\", (pos-1) / (double)(end-start)*1000);\n",
    "    }\n",
    "\n",
    "    free(prompt_tokens);\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5df016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void read_stdin(const char* guide, char* buffer, size_t bufsize) \n",
    "{\n",
    "    // read a line from stdin, up to but not including \\n\n",
    "    printf(\"%s\", guide);\n",
    "    if (fgets(buffer, bufsize, stdin) != NULL) {\n",
    "        size_t len = strlen(buffer);\n",
    "        if (len > 0 && buffer[len - 1] == '\\n') {\n",
    "            buffer[len - 1] = '\\0'; // strip newline\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74929da7",
   "metadata": {},
   "source": [
    "\n",
    "## Chat loop\n",
    "\n",
    "* manually inspected the tokens for a few chat conversations compared to\n",
    "* python reference and that seemed ok, but this was not thoroughly tested and\n",
    "* is not safely implemented, it's more a proof of concept at the moment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "void chat(  Transformer *transformer, \n",
    "            Tokenizer *tokenizer, \n",
    "            Sampler *sampler,\n",
    "            char *cli_user_prompt, \n",
    "            char *cli_system_prompt, \n",
    "            int steps) \n",
    "{\n",
    "\n",
    "    // buffers for reading the system prompt and user prompt from stdin\n",
    "    // you'll notice they are soomewhat haphazardly and unsafely set atm\n",
    "    \n",
    "    char system_prompt[   512];\n",
    "    char user_prompt[     512];\n",
    "    char rendered_prompt[1152];\n",
    "    int num_prompt_tokens = 0;\n",
    "    int* prompt_tokens = (int*)malloc(1152 * sizeof(int));\n",
    "    int user_idx;\n",
    "\n",
    "    // start the main loop\n",
    "    int8_t user_turn = 1; // user starts\n",
    "    int next;             // will store the next token in the sequence\n",
    "    int token;            // stores the current token to feed into the transformer\n",
    "    int prev_token;\n",
    "    int pos = 0;          // position in the sequence\n",
    "    while (pos < steps) {\n",
    "\n",
    "        // when it is the user's turn to contribute tokens to the dialog...\n",
    "        if (user_turn) {\n",
    "            // get the (optional) system prompt at position 0\n",
    "            if (pos == 0) {\n",
    "                // at position 0, the user can also contribute a system prompt\n",
    "                if (cli_system_prompt == NULL) {\n",
    "                    // system prompt was not passed in, attempt to get it from stdin\n",
    "                    read_stdin(\"Enter system prompt (optional): \", system_prompt, sizeof(system_prompt));\n",
    "                } else {\n",
    "                    // system prompt was passed in, use it\n",
    "                    strcpy(system_prompt, cli_system_prompt);\n",
    "                }\n",
    "            }\n",
    "            // get the user prompt\n",
    "            if (pos == 0 && cli_user_prompt != NULL) {\n",
    "                // user prompt for position 0 was passed in, use it\n",
    "                strcpy(user_prompt, cli_user_prompt);\n",
    "            } else {\n",
    "                // otherwise get user prompt from stdin\n",
    "                read_stdin(\"User: \", user_prompt, sizeof(user_prompt));\n",
    "            }\n",
    "            // render user/system prompts into the Llama 2 Chat schema\n",
    "            if (pos == 0 && system_prompt[0] != '\\0') {\n",
    "                char system_template[] = \"[INST] <<SYS>>\\n%s\\n<</SYS>>\\n\\n%s [/INST]\";\n",
    "                sprintf(rendered_prompt, system_template, system_prompt, user_prompt);\n",
    "            } else {\n",
    "                char user_template[] = \"[INST] %s [/INST]\";\n",
    "                sprintf(rendered_prompt, user_template, user_prompt);\n",
    "            }\n",
    "            // encode the rendered prompt into tokens\n",
    "            encode(tokenizer, rendered_prompt, 1, 0, prompt_tokens, &num_prompt_tokens);\n",
    "            user_idx  = 0; // reset the user index\n",
    "            user_turn = 0;\n",
    "            printf(\"Assistant: \");\n",
    "        }\n",
    "\n",
    "        // determine the token to pass into the transformer next\n",
    "        if (user_idx < num_prompt_tokens) {\n",
    "            // if we are still processing the input prompt, force the next prompt token\n",
    "            token = prompt_tokens[user_idx++];\n",
    "        } else {\n",
    "            // otherwise use the next token sampled from previous turn\n",
    "            token = next;\n",
    "        }\n",
    "        // EOS (=2) token ends the Assistant turn\n",
    "        if (token == 2) { user_turn = 1; }\n",
    "\n",
    "        // forward the transformer to get logits for the next token\n",
    "        float* logits = forward(transformer, token, pos);\n",
    "        next          = sample( sampler, logits        );\n",
    "        pos++;\n",
    "\n",
    "        if (user_idx >= num_prompt_tokens && next != 2) {\n",
    "            // the Assistant is responding, so print its output\n",
    "            char* piece = decode(tokenizer, token, next);\n",
    "            safe_printf(piece); // same as printf(\"%s\", piece), but skips \"unsafe\" bytes\n",
    "            fflush(stdout);\n",
    "        }\n",
    "        if (next == 2) { printf(\"\\n\"); }\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    free(prompt_tokens);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3deb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## CLI\n",
    "\n",
    "* include only if not testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dde172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ifndef TESTING\n",
    "\n",
    "void error_usage() {\n",
    "    fprintf(stderr, \"Usage:   run <checkpoint> [options]\\n\");\n",
    "    fprintf(stderr, \"Example: run model.bin -n 256 -i \\\"Once upon a time\\\"\\n\");\n",
    "    fprintf(stderr, \"Options:\\n\");\n",
    "    fprintf(stderr, \"  -t <float>  temperature in [0,inf], default 1.0\\n\");\n",
    "    fprintf(stderr, \"  -p <float>  p value in top-p (nucleus) sampling in [0,1] default 0.9\\n\");\n",
    "    fprintf(stderr, \"  -s <int>    random seed, default time(NULL)\\n\");\n",
    "    fprintf(stderr, \"  -n <int>    number of steps to run for, default 256. 0 = max_seq_len\\n\");\n",
    "    fprintf(stderr, \"  -i <string> input prompt\\n\");\n",
    "    fprintf(stderr, \"  -z <string> optional path to custom tokenizer\\n\");\n",
    "    fprintf(stderr, \"  -m <string> mode: generate|chat, default: generate\\n\");\n",
    "    fprintf(stderr, \"  -y <string> (optional) system prompt in chat mode\\n\");\n",
    "    exit(EXIT_FAILURE);\n",
    "}\n",
    "\n",
    "\n",
    "//----------------------------------------------\n",
    "\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "    // default parameters\n",
    "    \n",
    "    char *checkpoint_path = NULL;    // e.g. out/model.bin\n",
    "    char *tokenizer_path  = \"tokenizer.bin\";\n",
    "    float temperature     = 1.0f;    // 0.0 = greedy deterministic. 1.0 = original. don't set higher\n",
    "    float topp            = 0.9f;    // top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower\n",
    "    int steps             = 256;     // number of steps to run for\n",
    "    char *prompt          = NULL;    // prompt string\n",
    "    unsigned long long rng_seed = 0; // seed rng with time by default\n",
    "    char *mode            = \"generate\";    // generate|chat\n",
    "    char *system_prompt   = NULL;    // the (optional) system prompt to use in chat mode\n",
    "    \n",
    "\n",
    "    // poor man's C argparse so we can override the defaults above from the command line\n",
    "    \n",
    "    if (argc >= 2) { checkpoint_path = argv[1]; } else { error_usage(); }\n",
    "    \n",
    "    for (int i = 2; i < argc; i+=2) {\n",
    "        \n",
    "        // do some basic validation\n",
    "        \n",
    "        if (i + 1 >= argc) { error_usage(); }        // must have arg after flag\n",
    "        if (argv[i][0] != '-') { error_usage(); }    // must start with dash\n",
    "        if (strlen(argv[i]) != 2) { error_usage(); } // must be -x (one dash, one letter)\n",
    "        \n",
    "        // read in the args\n",
    "        if (argv[i][1]      == 't') { temperature = atof(argv[i + 1]); }\n",
    "        else if (argv[i][1] == 'p') { topp = atof(argv[i + 1]); }\n",
    "        else if (argv[i][1] == 's') { rng_seed = atoi(argv[i + 1]); }\n",
    "        else if (argv[i][1] == 'n') { steps = atoi(argv[i + 1]); }\n",
    "        else if (argv[i][1] == 'i') { prompt = argv[i + 1]; }\n",
    "        else if (argv[i][1] == 'z') { tokenizer_path = argv[i + 1]; }\n",
    "        else if (argv[i][1] == 'm') { mode = argv[i + 1]; }\n",
    "        else if (argv[i][1] == 'y') { system_prompt = argv[i + 1]; }\n",
    "        else { error_usage(); }\n",
    "    }\n",
    "\n",
    "    // parameter validation/overrides\n",
    "    \n",
    "    if (rng_seed <= 0) rng_seed = (unsigned int)time(NULL);\n",
    "    if (temperature < 0.0) temperature = 0.0;\n",
    "    if (topp < 0.0 || 1.0 < topp) topp = 0.9;\n",
    "    if (steps < 0) steps = 0;\n",
    "    \n",
    "\n",
    "    // build the Transformer via the model .bin file\n",
    "    Transformer transformer;\n",
    "    build_transformer(&transformer, checkpoint_path);\n",
    "    // ovrerride to ~max length\n",
    "    if (steps == 0 || steps > transformer.config.seq_len) steps = transformer.config.seq_len; \n",
    "    \n",
    "\n",
    "    \n",
    "    // build the Tokenizer via the tokenizer .bin file\n",
    "    Tokenizer tokenizer;\n",
    "    build_tokenizer(&tokenizer, tokenizer_path, transformer.config.vocab_size);\n",
    "\n",
    "    \n",
    "    // build the Sampler\n",
    "    Sampler sampler;\n",
    "    build_sampler(&sampler, transformer.config.vocab_size, temperature, topp, rng_seed);\n",
    "    \n",
    "\n",
    "    // run!\n",
    "    if (strcmp(mode, \"generate\")    == 0) {\n",
    "        generate(&transformer, &tokenizer, &sampler, prompt, steps);\n",
    "    } else if (strcmp(mode, \"chat\") == 0) {\n",
    "        chat(    &transformer, &tokenizer, &sampler, prompt, system_prompt, steps);\n",
    "    } else {\n",
    "        fprintf(stderr, \"unknown mode: %s\\n\", mode);\n",
    "        error_usage();\n",
    "    }\n",
    "\n",
    "    // memory and file handles cleanup\n",
    "    free_sampler(    &sampler    );\n",
    "    free_tokenizer(  &tokenizer  );\n",
    "    free_transformer(&transformer);\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "#endif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867056a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f6bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ef597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a0447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72fe7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a1f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9364cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294baae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299b4a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a19e233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
